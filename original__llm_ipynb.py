# -*- coding: utf-8 -*-
"""Копия блокнота "LLM.ipynb"

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-7bR0WfQ0NyFdNbmmWOoM2wDms6ytWKH
"""

!pip install -Uqqq pip --progress-bar off
!pip install -qqq torch==2.0.1 --progress-bar off
!pip install -qqq transformers==4.33.2 --progress-bar off
!pip install -qqq langchain==0.0.299 --progress-bar off
!pip install -qqq chromadb==0.4.10 --progress-bar off
!pip install -qqq xformers==0.0.21 --progress-bar off
!pip install -qqq sentence_transformers==2.2.2 --progress-bar off
!pip install -qqq tokenizers==0.14.0 --progress-bar off
!pip install -qqq optimum==1.13.1 --progress-bar off
!pip install -qqq auto-gptq==0.4.2 --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ --progress-bar off
!pip install -qqq unstructured==0.10.16 --progress-bar off

# hf_token = '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'
# import os

# os.environ['HUGGINGFACEHUB_API_TOKEN'] = hf_token



!pip install chromadb -q
!pip install langchain -q
!pip install sentence_transformers -q

import numpy as np
import pandas as pd

from langchain.document_loaders import DataFrameLoader
import pandas as pd

df = pd.read_csv('total_reports.csv', sep=';', encoding='utf-8')

df.head(3)

df = df.loc[:, 'Текст'].to_frame()

loader = DataFrameLoader(df, page_content_column="Текст")

from langchain.vectorstores import Chroma

df_document = loader.load()

from langchain.text_splitter import CharacterTextSplitter

text_splitter = CharacterTextSplitter(chunk_size=250, chunk_overlap=10)
texts = text_splitter.split_documents(df_document)

from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings
embedding_function = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

chromadb_index = Chroma.from_documents(
    texts, embedding_function, persist_directory='./input'
)

from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline

retriever = chromadb_index.as_retriever()

# model_id = "databricks/dolly-v2-3b" #my favourite textgeneration model for testing
# task="text-generation"

# HuggingFacePipeline.from_model_id?

# pip install auto-gptq optimum

import torch
from langchain import HuggingFacePipeline
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, pipeline

MODEL_NAME = "TheBloke/Llama-2-13b-Chat-GPTQ"

# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME, torch_dtype=torch.float16, trust_remote_code=True, device_map="auto"
)

# generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
# generation_config.max_new_tokens = 1024
# generation_config.temperature = 0.0001
# generation_config.top_p = 0.95
# generation_config.do_sample = True
# generation_config.repetition_penalty = 1.15

# text_pipeline = pipeline(
#     "text-generation",
#     model=model,
#     tokenizer=tokenizer,
#     generation_config=generation_config,
# )

# llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={"temperature": 0})

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)

generation_config = GenerationConfig.from_pretrained(MODEL_NAME)
generation_config.max_new_tokens = 1024
generation_config.temperature = 0.0001
generation_config.top_p = 0.95
generation_config.do_sample = True
generation_config.repetition_penalty = 1.15

text_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    generation_config=generation_config,
)

llm = HuggingFacePipeline(pipeline=text_pipeline, model_kwargs={"temperature": 0})

document_qa = RetrievalQA.from_chain_type(
    llm=llm, chain_type="stuff", retriever=retriever
)

response = document_qa.run("Инструкция: Предоставь информацию в форме одного предложения. " \
                           "Ответь на следующий вопрос: "\
                           "В каком месяце больше всего жалоб на грязь на улицах?" )
display(response)

# ------------------------------------------------------------------------------------------------------------------------------







llm(
    'Выдели адрес в следующем тексте: "Добрый день.Александр Дмитриевич.<br>По адресу ул.Подвойского 20к1, подъезд номер 3, находящийся в ведении ООО "ЖКС №1 Невского района" регулярно открыт подвал. Присутствуют постоянные обращения от жителей подъезда на портал "Наш Санкт-Петербург".<br>Также из-за свободного доступа в подвал там находятся лица без определенного места жительства. ЖКС №1 Абсолютно не следит за состоянием подъезда, в подъезде стоит ужасный запах. Решения проблемы не происходит. Надеюсь,вашим авторитетным мнением,получится заставить ООО "ЖКС №1 Невского района" соблюдать ЖК РФ,тем самым обеспечить безопасное и комфортное нахождение в подъезде для жителей.<br>Заранее благодарю за содействие в решение проблемы".'
)

llm(
    "Выдели указанный адрес в следующем тексте в формате для тренировка BERT модели: рядом с улицей Авиаторов Балтики 36 красивый парк и памятник"
)



from langchain import PromptTemplate

template = """
<s>[INST] <<SYS>>
Act as a Machine Learning engineer who is teaching high school students.
<</SYS>>

{text} [/INST]
"""

prompt = PromptTemplate(
    input_variables=["text"],
    template=template,
)

text = "Explain what are Deep Neural Networks in 2-3 sentences"
print(prompt.format(text=text))

result = llm(prompt.format(text=text))
print(result)

from langchain.chains import LLMChain

chain = LLMChain(llm=llm, prompt=prompt)
result = chain.run(text)
print(result)

template = "<s>[INST] Use the summary {summary} and give 3 examples of practical applications with 1 sentence explaining each [/INST]"

examples_prompt = PromptTemplate(
    input_variables=["summary"],
    template=template,
)
examples_chain = LLMChain(llm=llm, prompt=examples_prompt)

from langchain.chains import SimpleSequentialChain

multi_chain = SimpleSequentialChain(chains=[chain, examples_chain], verbose=True)
result = multi_chain.run(text)
print(result.strip())

